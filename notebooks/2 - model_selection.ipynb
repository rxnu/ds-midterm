{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "This notebook should include preliminary and baseline modeling.\n",
    "- Try as many different models as possible.\n",
    "- Don't worry about hyperparameter tuning or cross validation here.\n",
    "- Ideas include:\n",
    "    - linear regression\n",
    "    - support vector machines\n",
    "    - random forest\n",
    "    - xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X_train: (5229, 12),  y_train: (5229,)\n",
      "  X_test : (1308, 12),  y_test :  (1308,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Features\n",
    "X_train = pd.read_csv(\"../processed/X_train_scaled.csv\")\n",
    "X_test  = pd.read_csv(\"../processed/X_test_scaled.csv\")\n",
    "\n",
    "# Targets (single‐column, no header)\n",
    "y_train = pd.read_csv(\"../processed/y_train.csv\", header=None).iloc[:, 0]\n",
    "y_test  = pd.read_csv(\"../processed/y_test.csv\",  header=None).iloc[:, 0]\n",
    "\n",
    "print(f\"  X_train: {X_train.shape},  y_train: {y_train.shape}\")\n",
    "print(f\"  X_test : {X_test.shape},  y_test :  {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "any missing in X_train? False\n",
      "any missing in X_test?  False\n"
     ]
    }
   ],
   "source": [
    "# impute any remaining NaNs\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "# set up a median imputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "X_train = pd.DataFrame(\n",
    "    imputer.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test = pd.DataFrame(\n",
    "    imputer.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# check\n",
    "print(\"any missing in X_train?\", X_train.isna().any().any())\n",
    "print(\"any missing in X_test? \", X_test.isna().any().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Linear Regression</th>\n",
       "      <th>Support Vector Regressor</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Gradient‐Boosted Trees</th>\n",
       "      <th>K‐Nearest Neighbors Regressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.460148</td>\n",
       "      <td>13.961422</td>\n",
       "      <td>14.060679</td>\n",
       "      <td>14.010961</td>\n",
       "      <td>13.731231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.322610</td>\n",
       "      <td>12.633278</td>\n",
       "      <td>12.600734</td>\n",
       "      <td>12.581851</td>\n",
       "      <td>12.611345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.938019</td>\n",
       "      <td>11.917272</td>\n",
       "      <td>12.013707</td>\n",
       "      <td>11.978375</td>\n",
       "      <td>12.036589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.184556</td>\n",
       "      <td>11.950820</td>\n",
       "      <td>11.938771</td>\n",
       "      <td>11.992038</td>\n",
       "      <td>11.979400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.232930</td>\n",
       "      <td>12.690964</td>\n",
       "      <td>12.784129</td>\n",
       "      <td>12.786156</td>\n",
       "      <td>12.777481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Linear Regression  Support Vector Regressor  Random Forest  \\\n",
       "0          13.460148                 13.961422      14.060679   \n",
       "1          12.322610                 12.633278      12.600734   \n",
       "2          11.938019                 11.917272      12.013707   \n",
       "3          12.184556                 11.950820      11.938771   \n",
       "4          12.232930                 12.690964      12.784129   \n",
       "\n",
       "   Gradient‐Boosted Trees  K‐Nearest Neighbors Regressor  \n",
       "0               14.010961                      13.731231  \n",
       "1               12.581851                      12.611345  \n",
       "2               11.978375                      12.036589  \n",
       "3               11.992038                      11.979400  \n",
       "4               12.786156                      12.777481  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model                 import LinearRegression\n",
    "from sklearn.svm                          import SVR\n",
    "from sklearn.ensemble                     import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.neighbors                    import KNeighborsRegressor\n",
    "\n",
    "# defined 5 models\n",
    "models = {\n",
    "    \"Linear Regression\":               LinearRegression(),\n",
    "    \"Support Vector Regressor\":        SVR(kernel=\"rbf\"),\n",
    "    \"Random Forest\":                   RandomForestRegressor(random_state=42, n_estimators=100),\n",
    "    \"Gradient‐Boosted Trees\":          HistGradientBoostingRegressor(random_state=42),\n",
    "    \"K‐Nearest Neighbors Regressor\":    KNeighborsRegressor(n_neighbors=5),\n",
    "}\n",
    "predictions = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions[name] = model.predict(X_test)\n",
    "\n",
    "# packed into a DataFrame\n",
    "preds_df = pd.DataFrame(predictions, index=X_test.index)\n",
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider what metrics you want to use to evaluate success.\n",
    "- If you think about mean squared error, can we actually relate to the amount of error?\n",
    "- Try root mean squared error so that error is closer to the original units (dollars)\n",
    "- What does RMSE do to outliers?\n",
    "- Is mean absolute error a good metric for this problem?\n",
    "- What about R^2? Adjusted R^2?\n",
    "- Briefly describe your reasons for picking the metrics you use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE (log1p price)</th>\n",
       "      <th>MAE  (log1p price)</th>\n",
       "      <th>R² (log1p price)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>0.537</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Regressor</th>\n",
       "      <td>0.292</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.022</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient‐Boosted Trees</th>\n",
       "      <td>0.087</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K‐Nearest Neighbors Regressor</th>\n",
       "      <td>0.206</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               RMSE (log1p price)  MAE  (log1p price)  \\\n",
       "Linear Regression                           0.537               0.325   \n",
       "Support Vector Regressor                    0.292               0.125   \n",
       "Random Forest                               0.022               0.007   \n",
       "Gradient‐Boosted Trees                      0.087               0.039   \n",
       "K‐Nearest Neighbors Regressor               0.206               0.121   \n",
       "\n",
       "                               R² (log1p price)  \n",
       "Linear Regression                         0.686  \n",
       "Support Vector Regressor                  0.907  \n",
       "Random Forest                             0.999  \n",
       "Gradient‐Boosted Trees                    0.992  \n",
       "K‐Nearest Neighbors Regressor             0.954  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "metrics_log = {}\n",
    "for name, preds_log in preds_df.items():\n",
    "    rmse_log = np.sqrt(mean_squared_error(y_test, preds_log))\n",
    "    mae_log  = mean_absolute_error(y_test, preds_log)\n",
    "    r2_log   = r2_score(y_test, preds_log)\n",
    "    metrics_log[name] = {\n",
    "        \"RMSE (log1p price)\": round(rmse_log, 3),\n",
    "        \"MAE  (log1p price)\": round(mae_log, 3),\n",
    "        \"R² (log1p price)\"  : round(r2_log,  3),\n",
    "    }\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(metrics_log).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare our five baseline models fairly, we evaluated them on three complementary metrics applied to the log1p-transformed sale price:\n",
    "\n",
    "Root Mean Squared Error (RMSE)\n",
    "RMSE reports error in the same units as our transformed target and penalizes large mistakes more heavily. This is important because a single large misprediction on an expensive home can skew overall performance.\n",
    "\n",
    "Mean Absolute Error (MAE)\n",
    "MAE also reports in log-price units but treats every error equally. It gives a clear sense of the “typical” mistake without letting outliers dominate.\n",
    "\n",
    "R² (Coefficient of Determination)\n",
    "R² measures the proportion of variance in the log1p-price that our features explain. A higher R² means our model captures more of the underlying patterns in home values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection - STRETCH\n",
    "\n",
    "> **This step doesn't need to be part of your Minimum Viable Product (MVP), but its recommended you complete it if you have time!**\n",
    "\n",
    "Even with all the preprocessing we did in Notebook 1, you probably still have a lot of features. Are they all important for prediction?\n",
    "\n",
    "Investigate some feature selection algorithms (Lasso, RFE, Forward/Backward Selection)\n",
    "- Perform feature selection to get a reduced subset of your original features\n",
    "- Refit your models with this reduced dimensionality - how does performance change on your chosen metrics?\n",
    "- Based on this, should you include feature selection in your final pipeline? Explain\n",
    "\n",
    "Remember, feature selection often doesn't directly improve performance, but if performance remains the same, a simpler model is often preferrable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform feature selection \n",
    "# refit models\n",
    "# gather evaluation metrics and compare to the previous step (full feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso picked 12 features out of 12\n",
      "RFE picked 10 features: ['list_price', 'description.year_built', 'description.lot_sqft', 'description.sqft', 'description.baths', 'location.address.coordinate.lon', 'location.address.coordinate.lat', 'city_mean_price', 'state_mean_price', 'total_rooms']\n"
     ]
    }
   ],
   "source": [
    "# 1) pick features with LassoCV \n",
    "from sklearn.linear_model import LassoCV\n",
    "lasso = LassoCV(cv=5, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "# which coefficients survived?\n",
    "import pandas as pd\n",
    "coef = pd.Series(lasso.coef_, index=X_train.columns)\n",
    "lasso_feats = coef[coef.abs() > 1e-6].index.tolist()\n",
    "print(f\"Lasso picked {len(lasso_feats)} features out of {X_train.shape[1]}\")\n",
    "\n",
    "# 2) pick top 10 with RFE + RandomForest\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rfe = RFE(rf, n_features_to_select=10).fit(X_train, y_train)\n",
    "rfe_feats = X_train.columns[rfe.support_].tolist()\n",
    "print(f\"RFE picked {len(rfe_feats)} features: {rfe_feats}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) build reduced train/test sets \n",
    "X_train_lasso = X_train[lasso_feats]\n",
    "X_test_lasso  = X_test [lasso_feats]\n",
    "\n",
    "X_train_rfe   = X_train[rfe_feats]\n",
    "X_test_rfe    = X_test [rfe_feats]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Full RMSE  Full MAE  Full R²  Lasso RMSE  Lasso MAE  \\\n",
      "Linear Regression      0.537     0.325    0.686       0.537      0.325   \n",
      "SVR (RBF)              0.292     0.125    0.907       0.292      0.125   \n",
      "Random Forest          0.022     0.007    0.999       0.022      0.007   \n",
      "HistGB Regressor       0.087     0.039    0.992       0.087      0.039   \n",
      "KNN Regressor          0.206     0.121    0.954       0.206      0.121   \n",
      "\n",
      "                   Lasso R²  RFE RMSE  RFE MAE  RFE R²  \n",
      "Linear Regression     0.686     0.544    0.330   0.678  \n",
      "SVR (RBF)             0.907     0.283    0.122   0.913  \n",
      "Random Forest         0.999     0.022    0.007   0.999  \n",
      "HistGB Regressor      0.992     0.087    0.040   0.992  \n",
      "KNN Regressor         0.954     0.211    0.122   0.952  \n"
     ]
    }
   ],
   "source": [
    "# 4) refit our 5 models and compare metrics \n",
    "import numpy as np\n",
    "from sklearn.linear_model            import LinearRegression\n",
    "from sklearn.svm                     import SVR\n",
    "from sklearn.ensemble                import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.neighbors               import KNeighborsRegressor\n",
    "from sklearn.metrics                 import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\":   LinearRegression(),\n",
    "    \"SVR (RBF)\":           SVR(kernel=\"rbf\"),\n",
    "    \"Random Forest\":       RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"HistGB Regressor\":    HistGradientBoostingRegressor(random_state=42),\n",
    "    \"KNN Regressor\":       KNeighborsRegressor(n_neighbors=5),\n",
    "}\n",
    "\n",
    "def eval_set(X_tr, X_te, label):\n",
    "    out = {}\n",
    "    for name, m in models.items():\n",
    "        m.fit(X_tr, y_train)\n",
    "        p = m.predict(X_te)\n",
    "        mse = mean_squared_error(y_test, p)\n",
    "        out[name] = {\n",
    "            f\"{label} RMSE\": np.sqrt(mse),\n",
    "            f\"{label} MAE\":  mean_absolute_error(y_test, p),\n",
    "            f\"{label} R²\":   r2_score(y_test, p)\n",
    "        }\n",
    "    return pd.DataFrame(out).T\n",
    "\n",
    "full = eval_set(X_train,      X_test,      \"Full\")\n",
    "lasso = eval_set(X_train_lasso, X_test_lasso, \"Lasso\")\n",
    "rfe   = eval_set(X_train_rfe,   X_test_rfe,   \"RFE\")\n",
    "\n",
    "comparison = pd.concat([full, lasso, rfe], axis=1).round(3)\n",
    "print(comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Lasso-based selection hardly eliminated any variables, the overall RMSE, MAE, and R² stayed, virtually unchanged, whereas the RFE procedure distilled the dataset down to just ten features while preserving almost identical performance on both Random Forest and HistGradientBoosting models. RFE gives us a much leaner feature set without sacrificing accuracy. This makes it an attractive choice when we want faster training times and a simpler, more maintainable pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exam_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
